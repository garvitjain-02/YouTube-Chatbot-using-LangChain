{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WOcmRAWu5KV3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# List available models\n",
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LjGWL7rz9RP",
        "outputId": "d0c78592-d8a8-4da7-9444-362f6fbc41ef"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv langchain-google-genai google-ai-generativelanguage==0.6.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4JwVw0E64g8",
        "outputId": "11f3390b-c7bb-4792-e9b4-b1648259662a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/2.2 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "9CTf04PB69Z6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Document Ingestion*"
      ],
      "metadata": {
        "id": "UQ4YeH2H7rE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NFqKRmyg-Ia9",
        "outputId": "1f198442-e964-40ee-8e23-0fc9e2d632bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=6278bbc2e10017906aa2af5bf9690a3216490b6642d1b3079491d72332a4814c\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pytube, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-dl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJoGt-PT-5VI",
        "outputId": "ac05611f-bd09-43de-f2cd-f1fa5896e89c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2021.12.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQy-xx9z-v03",
        "outputId": "27645b69-6fc8-453a-add2-2ed777caa540"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pytube import YouTube\n",
        "\n",
        "# video_url = \"https://www.youtube.com/watch?v=wgfSDrqYMJ4\"\n",
        "# !yt-dlp -x --audio-format mp3 {video_url} -o \"audio.%(ext)s\"\n",
        "\n",
        "# The downloaded file should be named \"audio.mp3\" in the same directory as your notebook."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PxoZGaAu--o0",
        "outputId": "3c00e675-5bb7-4785-adef-2387641189c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.5.22-py3-none-any.whl.metadata (174 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/174.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/174.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2025.5.22-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.5.22\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=wgfSDrqYMJ4\n",
            "[youtube] wgfSDrqYMJ4: Downloading webpage\n",
            "[youtube] wgfSDrqYMJ4: Downloading tv client config\n",
            "[youtube] wgfSDrqYMJ4: Downloading player 612f74a3-main\n",
            "[youtube] wgfSDrqYMJ4: Downloading tv player API JSON\n",
            "[youtube] wgfSDrqYMJ4: Downloading ios player API JSON\n",
            "[youtube] wgfSDrqYMJ4: Downloading m3u8 information\n",
            "[info] wgfSDrqYMJ4: Downloading 1 format(s): 251\n",
            "[download] Destination: audio.webm\n",
            "\u001b[K[download] 100% of    6.61MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m24.64MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: audio.mp3\n",
            "Deleting original file audio.webm (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")  # Or \"small\", \"medium\", \"large\"\n",
        "result = model.transcribe(\"audio.mp3\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MomBcOUv-IXk",
        "outputId": "1765bc45-4a82-4ef0-b61e-ae261a67e079"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:03<00:00, 47.2MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A word, on word, embeddings. Maybe a few words. Word embeddings represent words as numbers, specifically as numeric vectors. In a way that captures their semantic relationships and contextual information. So that means words with similar meanings are positioned close to each other and the distance and direction between vectors encode the degree of similarity between words. But why do we need to transform words into numbers? The reason vectors are used to represent words is that most machine learning algorithms are just incapable of processing plain text in its raw form. They require numbers as input to perform any task. And that's where word embeddings come in. So let's take a look at how word embeddings are used and the model is used to create them. And let's start with a look at some applications. Now word embeddings have become a fundamental tool in the world of NLP. That's natural language processing. Natural language processing helps machines understand human language. Word embeddings are used in various NLP tasks. So for example, you'll find them used in text classification very frequently. Now in text classification, word embeddings are often used in tasks such as spam detection and topic categorization. Another common task is NER, a synacronym for named entity recognition. And there is used to identify and classify entities in text. And an entity is like a name of a person or a place or an organization. Now word embeddings can also help with tasks related to word similarity and word analogy tasks. So for example, the king is dequeen as man is to woman. And then another example is in Q&A. So question and answering systems, they can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related articles or finding similar documents or recommending similar items. Now word embeddings are created by training models on a large corpus of text. So maybe like all of Wikipedia. The process begins with pre-processing the text, including tokenization and removing stop words and punctuation, a sliding context window identifies target and context words, allowing the model to learn word relationships. Then the model is trained to predict based on a context, positioning semantically similar words close together in the vector space and throughout the training, the model parameters are adjusted to minimize prediction errors. So what does this look like? Well, let's start with a super small corpus of just six words. Here they are. Now we'll represent each word as a three-dimensional vector. So each dimension has a numeric value creating a unique vector for each word. And these values represent the word's position in a continuous three-dimensional vector space. And if you look closely, you can see the words with similar meanings or context have similar vector representations. So for instance, the vectors for Apple and for Orange are close together reflecting their semantic relationship. Likewise, the vectors for happy and sad have opposite directions indicating their contrasting meanings. Now of course, in real life, it's not this simple. A corpus of six words isn't going to be too helpful in practice. And actual word embeddings typically have hundreds of dimensions, not just three, to capture more intricate relationships and nuances in meaning. Now there are two fundamental approaches to how word embedding methods generate vector representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one, which is frequency. So frequency-based embeddings. Now frequency-based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now one such embedding of frequency-based is called T-F-I-D-F. That stands for term frequency inverse document frequency. T-F-I-D-F highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others, about different topics. Common words like V or AND, which you pay frequently across all documents, would receive low T-F-I-D-F-based scores. Now another embedding type is called prediction-based embeddings. And prediction-based embeddings, they capture semantic relationships and contextual information between words. So for example, in the sentences, the dog is barking loudly, and the dog is barking its tail. A prediction-based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various dog-related concepts. Prediction-based embeddings, they excel at separating words with close meanings and can manage the various senses in which a word may be used. Now there are various models for generating word embeddings. One of the most popular is called word to veg. That was developed by Google in 2013. Now word to veg has two main architectures. There's something called CBOW, and there's something called skip ground. And CBOW, that's an acronym for Continuous Bag of Words. Now CBOW predicts a target word based on its surrounding context words. Well, skip ground does the opposite, predicting the context words given a target word. Now another popular method is called glove, also an acronym. That one stands for global vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the sentence, I'm going to the bank to deposit money, and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language, proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world. We tab the values of анvent sorting every word that model in understanding our perception ofSPEAK point of view, lesson in meaning of calling world pens of surveying,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Text Splitting*"
      ],
      "metadata": {
        "id": "TBamWpTUBXWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript = result['text']\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ],
      "metadata": {
        "id": "xYEzy03Q-IVt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAsrCRCl-ITl",
        "outputId": "548a0985-3477-430c-9e16-02fc6e004061"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Embedding Generation and Storing in Vector Store*"
      ],
      "metadata": {
        "id": "Xz6SoGhdBfKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "ul2B_yd7-IQx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.index_to_docstore_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUPtrQdP77jx",
        "outputId": "7cf7a4f5-4e8a-4213-9d3d-dcd33a250fc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'da1a3391-debd-4c23-b3f4-7a82ec6962f9',\n",
              " 1: '6be3cf4b-0550-4940-9071-8c001cc3d83b',\n",
              " 2: 'eb44225e-1eea-451d-8d49-30c83e8303ff',\n",
              " 3: '2a3fed4b-65bd-4c69-9850-081a0776817e',\n",
              " 4: 'ffabf034-bfcb-4aa1-99c0-3813caf738a2',\n",
              " 5: 'fa25385f-9cc3-42aa-86a3-5e8f4c0be137',\n",
              " 6: '1e68ed8f-203d-4513-aa66-197fa704986e',\n",
              " 7: '5e41d36e-376e-47a4-8875-e1d4356e2a74',\n",
              " 8: '9e26f054-4a03-4449-b83f-7fc6d8db502c'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Retrieval*"
      ],
      "metadata": {
        "id": "Vmji5MsNB-yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "TPL4brCMB5gg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mR7lO5eCPuU",
        "outputId": "5c2a469a-1688-42e1-e129-84fb2e8dbb13"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x796ebc6acad0>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke('Applications of Word embeddings?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGiLmm7LCSVA",
        "outputId": "e7704e8e-6518-4757-fbb6-302f09ca64f5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='6be3cf4b-0550-4940-9071-8c001cc3d83b', metadata={}, page_content=\"applications. Now word embeddings have become a fundamental tool in the world of NLP. That's natural language processing. Natural language processing helps machines understand human language. Word embeddings are used in various NLP tasks. So for example, you'll find them used in text classification very frequently. Now in text classification, word embeddings are often used in tasks such as spam detection and topic categorization. Another common task is NER, a synacronym for named entity recognition. And there is used to identify and classify entities in text. And an entity is like a name of a person or a place or an organization. Now word embeddings can also help with tasks related to word similarity and word analogy tasks. So for example, the king is dequeen as man is to woman. And then another example is in Q&A. So question and answering systems, they can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related\"),\n",
              " Document(id='eb44225e-1eea-451d-8d49-30c83e8303ff', metadata={}, page_content=\"another example is in Q&A. So question and answering systems, they can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related articles or finding similar documents or recommending similar items. Now word embeddings are created by training models on a large corpus of text. So maybe like all of Wikipedia. The process begins with pre-processing the text, including tokenization and removing stop words and punctuation, a sliding context window identifies target and context words, allowing the model to learn word relationships. Then the model is trained to predict based on a context, positioning semantically similar words close together in the vector space and throughout the training, the model parameters are adjusted to minimize prediction errors. So what does this look like? Well, let's start with a super small corpus of just six words. Here they are. Now we'll represent each word as a three-dimensional vector. So each\"),\n",
              " Document(id='da1a3391-debd-4c23-b3f4-7a82ec6962f9', metadata={}, page_content=\"A word, on word, embeddings. Maybe a few words. Word embeddings represent words as numbers, specifically as numeric vectors. In a way that captures their semantic relationships and contextual information. So that means words with similar meanings are positioned close to each other and the distance and direction between vectors encode the degree of similarity between words. But why do we need to transform words into numbers? The reason vectors are used to represent words is that most machine learning algorithms are just incapable of processing plain text in its raw form. They require numbers as input to perform any task. And that's where word embeddings come in. So let's take a look at how word embeddings are used and the model is used to create them. And let's start with a look at some applications. Now word embeddings have become a fundamental tool in the world of NLP. That's natural language processing. Natural language processing helps machines understand human language. Word\"),\n",
              " Document(id='5e41d36e-376e-47a4-8875-e1d4356e2a74', metadata={}, page_content=\"global vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the\")]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Augmentation*"
      ],
      "metadata": {
        "id": "C9P0c4vGC6cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "M4bNDsPjCfVm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ],
      "metadata": {
        "id": "DtCzupqWDVGG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question          = \"Explain contextual based embeddings in regard of transcript\"\n",
        "retrieved_docs    = retriever.invoke(question)"
      ],
      "metadata": {
        "id": "_TuSyj__zPBk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCLwUHkhzgZK",
        "outputId": "527f51fe-04a7-4785-87df-fb5752065dc1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='9e26f054-4a03-4449-b83f-7fc6d8db502c', metadata={}, page_content=\"based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the sentence, I'm going to the bank to deposit money, and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language, proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world. We tab the values of анvent sorting every word that model in understanding our perception ofSPEAK point of view, lesson in meaning of calling world pens of surveying,\"),\n",
              " Document(id='fa25385f-9cc3-42aa-86a3-5e8f4c0be137', metadata={}, page_content='highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others, about different topics. Common words like V or AND, which you pay frequently across all documents, would receive low T-F-I-D-F-based scores. Now another embedding type is called prediction-based embeddings. And prediction-based embeddings, they capture semantic relationships and contextual information between words. So for example, in the sentences, the dog is barking loudly, and the dog is barking its tail. A prediction-based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various dog-related concepts. Prediction-based embeddings, they excel at separating words with close meanings and can manage the'),\n",
              " Document(id='5e41d36e-376e-47a4-8875-e1d4356e2a74', metadata={}, page_content=\"global vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the\"),\n",
              " Document(id='ffabf034-bfcb-4aa1-99c0-3813caf738a2', metadata={}, page_content=\"practice. And actual word embeddings typically have hundreds of dimensions, not just three, to capture more intricate relationships and nuances in meaning. Now there are two fundamental approaches to how word embedding methods generate vector representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one, which is frequency. So frequency-based embeddings. Now frequency-based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now one such embedding of frequency-based is called T-F-I-D-F. That stands for term frequency inverse document frequency. T-F-I-D-F highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or\")]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "aEJWj5YoziB4",
        "outputId": "95ca16cd-9ac9-4258-abdf-e83b6680746f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the sentence, I'm going to the bank to deposit money, and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language, proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world. We tab the values of анvent sorting every word that model in understanding our perception ofSPEAK point of view, lesson in meaning of calling world pens of surveying,\\n\\nhighlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others, about different topics. Common words like V or AND, which you pay frequently across all documents, would receive low T-F-I-D-F-based scores. Now another embedding type is called prediction-based embeddings. And prediction-based embeddings, they capture semantic relationships and contextual information between words. So for example, in the sentences, the dog is barking loudly, and the dog is barking its tail. A prediction-based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various dog-related concepts. Prediction-based embeddings, they excel at separating words with close meanings and can manage the\\n\\nglobal vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the\\n\\npractice. And actual word embeddings typically have hundreds of dimensions, not just three, to capture more intricate relationships and nuances in meaning. Now there are two fundamental approaches to how word embedding methods generate vector representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one, which is frequency. So frequency-based embeddings. Now frequency-based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now one such embedding of frequency-based is called T-F-I-D-F. That stands for term frequency inverse document frequency. T-F-I-D-F highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ],
      "metadata": {
        "id": "opFqvVQIzleq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Y17VDDzogS",
        "outputId": "1b89fd2f-3f93-4e78-d367-04c5234ced8f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the sentence, I'm going to the bank to deposit money, and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language, proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world. We tab the values of анvent sorting every word that model in understanding our perception ofSPEAK point of view, lesson in meaning of calling world pens of surveying,\\n\\nhighlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others, about different topics. Common words like V or AND, which you pay frequently across all documents, would receive low T-F-I-D-F-based scores. Now another embedding type is called prediction-based embeddings. And prediction-based embeddings, they capture semantic relationships and contextual information between words. So for example, in the sentences, the dog is barking loudly, and the dog is barking its tail. A prediction-based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various dog-related concepts. Prediction-based embeddings, they excel at separating words with close meanings and can manage the\\n\\nglobal vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the\\n\\npractice. And actual word embeddings typically have hundreds of dimensions, not just three, to capture more intricate relationships and nuances in meaning. Now there are two fundamental approaches to how word embedding methods generate vector representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one, which is frequency. So frequency-based embeddings. Now frequency-based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now one such embedding of frequency-based is called T-F-I-D-F. That stands for term frequency inverse document frequency. T-F-I-D-F highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or\\n      Question: Explain contextual based embeddings in regard of transcript\\n    \")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Generation*"
      ],
      "metadata": {
        "id": "_JeTs7n8zuZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_perwkTzpx-",
        "outputId": "7265d0bb-c7a5-4372-a3e0-ee9202431915"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contextual based embeddings are where the representation of a word changes based on its surrounding context. For example, in a transformer model, the word \"bank\" would have different representations in the sentence \"I'm going to the bank to deposit money\" and \"I'm sitting on the bank of a river\". This context sensitivity allows these models to capture more nuanced meanings and relationships between words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Chain"
      ],
      "metadata": {
        "id": "N0vjf2fo0eVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "_Cok50PWzyu3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "ISqioPcn1XF6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "QbNpIWwT1aWW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke('context based embedding')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-A_nOzo2Crp",
        "outputId": "94fa5788-383a-4820-b36e-43eb4d8aa387"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"global vectors for word representation. That was created at Stanford University in 2014. And that uses co-occurrent statistics to create word vectors. Now these models, they differ in their approach. Word to veg that focuses on learning from the immediate context around each word, while glove takes a broad view by analyzing how often words appear together across the entire corpus, then uses this information to create word vectors. Now while these two word embedding models continue to be valuable tools in NLP, the field has seen some significant advances with the emergence of new tech, particularly transformers. While traditional word embeddings assign a fixed vector to each word, transformer models use a different type of embedding, and it's called a contextual based embedding. Now contextual based embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the\\n\\nhighlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or cappuccino, which might appear often in that document, but rarely in others, about different topics. Common words like V or AND, which you pay frequently across all documents, would receive low T-F-I-D-F-based scores. Now another embedding type is called prediction-based embeddings. And prediction-based embeddings, they capture semantic relationships and contextual information between words. So for example, in the sentences, the dog is barking loudly, and the dog is barking its tail. A prediction-based model would learn to associate dog with words like bark, wag, and tail. This allows these models to create a single fixed representation for dog that encompasses various dog-related concepts. Prediction-based embeddings, they excel at separating words with close meanings and can manage the\\n\\nbased embeddings are whether representation of a word changes based on its surrounding context. So for example, in a transformer model, the word bank would have different representations in the sentence, I'm going to the bank to deposit money, and I'm sitting on the bank of a river. This context sensitivity allows these models to capture more nuanced meanings and relationships between words, which has led to all sorts of improvements in the various fields of NLP tasks. So that's word embeddings from simple numeric vectors to complex representations. Word embeddings have revolutionized how machines understand and process human language, proving that transforming words into numbers is indeed a powerful tool for making sense of our linguistic world. We tab the values of анvent sorting every word that model in understanding our perception ofSPEAK point of view, lesson in meaning of calling world pens of surveying,\\n\\npractice. And actual word embeddings typically have hundreds of dimensions, not just three, to capture more intricate relationships and nuances in meaning. Now there are two fundamental approaches to how word embedding methods generate vector representations for words. So let's take a look at some of these embedding methods. And we'll start with the first one, which is frequency. So frequency-based embeddings. Now frequency-based embeddings are word representations that are derived from the frequency of words in a corpus. They're based on the idea that the importance or the significance of a word can be inferred from how frequently it occurs in the text. Now one such embedding of frequency-based is called T-F-I-D-F. That stands for term frequency inverse document frequency. T-F-I-D-F highlights words that are frequent within a specific document, but are rare across the entire corpus. So for example, in a document about coffee, T-F-I-D-F would emphasize words like espresso or\",\n",
              " 'question': 'context based embedding'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "LT1xt8ze2JOw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "SdSq-j-P2QWq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "bv0cYm7f2Uv8",
        "outputId": "c198bc04-3c24-47f0-e09c-4f7387b63769"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video discusses word embeddings, which transform words into numbers to help machines understand and process human language. It covers different types of embeddings, including T-F-I-D-F and prediction-based embeddings, explaining how they capture semantic relationships and contextual information. The video also touches on context-sensitive embeddings, where the representation of a word changes based on its surrounding context, as seen in transformer models. It uses examples like \"apple\" and \"orange\" or \"happy\" and \"sad\" to illustrate semantic relationships and explains how models are trained to minimize prediction errors, using a small corpus of six words as an example. The video further suggests that question and answering systems can benefit from word embeddings for measuring semantic similarities between words or documents for tasks like clustering related articles or finding similar documents or recommending similar items.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5PwdQ1W2YDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}